# 最基本的神经网络计算过程
using Flux
m=Dense(2,1)
ps=params(m)
ps.order[1] .= [0.2 0.3] #强制设定初始参数
w=ps.order[1]
b=ps.order[2]

x=[1.0,2.0]
y=1.0
loss(x,y)=Flux.mse(m(x),y)
loss(x,y) #0.03999999523162856 ≃ 0.04  (w1*x1+w2*x2+b-y)^2=(0.2*1+0.3*2+0-1)^2=0.04

# 计算梯度，所谓梯度，指的是损失函数对参数的导数。如果是深度神经网络，则需要考虑复合函数求导法则
grads = gradient(() -> loss(x, y), ps)
grads[w] #-0.4 -0.8 见下面的分析
grads[b] #-0.3999999761581421 ≃ -0.4 

η = 0.1 # Learning Rate
for p in ps
  Flux.update!(p, η * grads[p])
end
println(w)
println(b)

#=以上计算的理论过程如下
z=(w1*x1+w2*x2-y)^2
∂z/∂w1=2(w1*x1+w2*x2+b-y)*x1=2(0.2*1+0.3*2-1)*1=-0.4
∂z/∂w2=2(w1*x1+w2*x2+b-y)*x2=2(0.2*1+0.3*2-1)*2=-0.8
∂z/∂b =2(w1*x1+w2*x2+b-y)*1 =2(0.2*1+0.3*2-1)*2=-0.4
w1=w1-0.1*∂z/∂w1=0.2-(-0.4*0.1)=0.24
w2=w2-0.1*∂z/∂w2=0.3-(-0.8*0.1)=0.38
b=b-0.1*∂z/∂b=0-(-0.4*0.1)=0.04
=#
